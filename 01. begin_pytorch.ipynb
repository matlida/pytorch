{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1）使用pytorch创建一个tensor\n",
    "# 2）tensor的基本操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.]), tensor([0., 0.]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 一个tensor可以有不用的维度，可以是1维的，2维的，也可以是3维，4维的，我们先来创建一个空的tensor\n",
    "\n",
    "x = torch.empty(1)\n",
    "y = torch.empty(2)\n",
    "x,y\n",
    "\n",
    "# 括号里面我们要给tensor一个size，空的tensor不代表是空的张量，可以看到x只有一个元素，y里面有3个元素，y可以理解维1维的向量包含2个元素\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.],\n",
       "         [0.]]),\n",
       " tensor([[0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 0.0000e+00],\n",
       "         [0.0000e+00, 7.8473e-44]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在我们创建一个2维的tensor\n",
    "\n",
    "x = torch.empty(2,1)\n",
    "y = torch.empty(3,2)\n",
    "x,y\n",
    "# 他看起来像一个二维的矩阵，也有些像三行两列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.0000e+00, 0.0000e+00, 0.0000e+00]],\n",
       " \n",
       "         [[0.0000e+00, 0.0000e+00, 7.8473e-44]]]),\n",
       " tensor([[[0.0000e+00],\n",
       "          [8.1275e-44]],\n",
       " \n",
       "         [[2.7003e-06],\n",
       "          [2.6705e-06]],\n",
       " \n",
       "         [[8.4488e+20],\n",
       "          [4.2724e-05]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在我们创建一个3维的tensor\n",
    "\n",
    "x = torch.empty(2,1,3)\n",
    "y = torch.empty(3,2,1)\n",
    "x,y\n",
    "# 他看起来像一个二维的矩阵，也有些像三行两列的矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2676],\n",
       "         [0.3381]]),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.],\n",
       "         [1., 1.]]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以用随机值来生成tesor，或者生成一个全为0/1的tensor\n",
    " \n",
    "\n",
    "x = torch.rand(2,1)\n",
    "y = torch.zeros(3,2)\n",
    "z  = torch.ones(3,2)\n",
    "x,y,z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以指定我们的数据的类型，先来看一下默认的数据类型\n",
    "\n",
    "x = torch.rand(2,1)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以指定我们的数据的类型 eg.double,folat16\n",
    "\n",
    "x = torch.rand(2,1,dtype=torch.float64)\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以查看我们的数据的size,因为size是一个函数，所以产看size的时候需要加括号\n",
    "\n",
    "x = torch.rand(2,1,dtype=torch.float64)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 1, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们也可以将从其他的python的数据类型eg我们的列表或者array转化成我们的tensor\n",
    "x = torch.tensor([2,1,9])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3290, 0.4252],\n",
       "         [0.7578, 0.1285]]),\n",
       " tensor([[0.8928, 0.0837],\n",
       "         [0.1531, 0.2310]]),\n",
       " tensor([[1.2219, 0.5089],\n",
       "         [0.9109, 0.3595]]),\n",
       " tensor([[1.2219, 0.5089],\n",
       "         [0.9109, 0.3595]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来我们来看我们的tesor的基本的运算，比如说我们的加减乘除\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x+y\n",
    "z1 = torch.add(x,y)\n",
    "x,y,z,z1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.2090, 1.7847],\n",
       "        [3.1843, 0.7452]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 注意如果我们使用以下这种方式，那么我的变量y会变成 x+y，在pytorch中，如果在一个函数后面加一个下划线的话，那个就相当于使用 = \n",
    "y.add_(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4969, 0.4190],\n",
       "         [0.6998, 0.3150]]),\n",
       " tensor([[0.3287, 0.3284],\n",
       "         [0.7985, 0.3828]]),\n",
       " tensor([[ 0.1682,  0.0905],\n",
       "         [-0.0987, -0.0678]]),\n",
       " tensor([[ 0.1682,  0.0905],\n",
       "         [-0.0987, -0.0678]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然减法也可以使用这种方式\n",
    "\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x-y\n",
    "z1 = torch.sub(x,y)\n",
    "x,y,z,z1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7932, 0.1408],\n",
       "         [0.1936, 0.6415]]),\n",
       " tensor([[0.3042, 0.2218],\n",
       "         [0.5851, 0.5936]]),\n",
       " tensor([[0.2413, 0.0312],\n",
       "         [0.1133, 0.3808]]),\n",
       " tensor([[0.2413, 0.0312],\n",
       "         [0.1133, 0.3808]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然乘法也可以使用这种方式\n",
    "\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x*y\n",
    "z1 = torch.mul(x,y)\n",
    "x,y,z,z1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7676, 0.8390],\n",
       "         [0.7951, 0.0676]]),\n",
       " tensor([[0.1735, 0.9877],\n",
       "         [0.4220, 0.3648]]),\n",
       " tensor([[4.4247, 0.8495],\n",
       "         [1.8844, 0.1853]]),\n",
       " tensor([[4.4247, 0.8495],\n",
       "         [1.8844, 0.1853]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 当然乘法也可以使用这种方式\n",
    "\n",
    "x = torch.rand(2,2)\n",
    "y = torch.rand(2,2)\n",
    "z = x/y\n",
    "z1 = torch.div(x,y)\n",
    "x,y,z,z1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 以上就是我们的tensor的基础运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9377, 0.9684, 0.5864],\n",
       "         [0.1396, 0.1496, 0.5913],\n",
       "         [0.2814, 0.8852, 0.0992],\n",
       "         [0.1747, 0.1173, 0.7173],\n",
       "         [0.1256, 0.9443, 0.9350]]),\n",
       " tensor([0.9684, 0.1496, 0.8852, 0.1173, 0.9443]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来我们可以像numpy或者df一样对我们的tesor进行切片\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "# 这里比如说我们取所有的行第一列的数据，或者第一行，或者取一个元素\n",
    "x,x[:,1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 我们可以对我们的tensor进行reshape，但是注意reshape的时候元素的个数是一样的\n",
    "x = torch.rand(4,4)\n",
    "y = x.view(16)\n",
    "x,y\n",
    "# 现在可以看到我们的y只有1维的tensor \n",
    "# 如果我们不想输入是size的第一个值，也就是维度，我们可以用-1来代替，他会自动的帮我们算出来\n",
    "\n",
    "z = x.view(-1,8)\n",
    "z.size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1.]), array([1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 现在我们来讨论一下numpy和tensor的互相转换,我们先来创建一个tensor\n",
    "a = torch.ones(5)\n",
    "b = a.numpy()\n",
    "a,b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2.]), array([2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里如果你的tensor和numpy都是在cup上存储时，那么这个变量a和b是在内存中使用同一个地址的，如果说你修改了一个，那么另一个也是会修改掉的，eg\n",
    "a.add_(1)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 1., 1., 1., 1.]),\n",
       " tensor([1., 1., 1., 1., 1.], dtype=torch.float64))"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 接下来我们来创建一个numpy\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2.]), array([2., 2., 2., 2., 2.], dtype=float32))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 同理如果你修改了a的值，b的值也会被修改\n",
    "a += 1\n",
    "a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 如果你的gpu是可用的，tensor是存放在gpu上的，可以使用下面的代码在gpu上创建tensor\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    x = torch.ones(5,device = device)\n",
    "    y = torch.ones(5)\n",
    "    y = y.to(device)\n",
    "    z = x + y\n",
    "# 一般来说在gpu上的计算速度会比在cpu上的速度快很多，这里注意tensor是可以放在gpu的，而我们的numpy是只能存储在cpu的，所以如果要将z转换成numpy，需要将tensor先放到我们的cpu上。\n",
    "    z = z.to(\"cpu\")\n",
    "    z.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1.], requires_grad=True)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 如果你有一个参数要优化，需要用到梯度下降法，当我们在创建一个tensor的时候，我们可以直接输入~为true，这样他会自动的帮我们计算梯度\n",
    "\n",
    "x = torch.ones(5,requires_grad=True)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "49fca704c6919c58f6aea1e6d1baec6a2f11de6394a6e4f591f27d75015a902f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
